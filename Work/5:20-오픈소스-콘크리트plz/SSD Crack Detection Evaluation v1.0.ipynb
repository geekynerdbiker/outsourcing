{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (C) 2018 Pierluigi Ferrari\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    " Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "from scipy.misc import imread\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from models.keras_ssd300 import ssd_300\n",
    "from keras_loss_function.keras_ssd_loss import SSDLoss\n",
    "from keras_layers.keras_layer_AnchorBoxes import AnchorBoxes\n",
    "from keras_layers.keras_layer_DecodeDetections import DecodeDetections\n",
    "from keras_layers.keras_layer_DecodeDetectionsFast import DecodeDetectionsFast\n",
    "from keras_layers.keras_layer_L2Normalization import L2Normalization\n",
    "from data_generator.object_detection_2d_data_generator import DataGenerator\n",
    "from eval_utils.average_precision_evaluator import Evaluator\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 300\n",
    "img_width = 300\n",
    "n_classes = 1\n",
    "model_mode = 'inference'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. SSD Model을 설계하는 코드입니다.\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "model = ssd_300(image_size=(img_height, img_width, 3),\n",
    "                n_classes=n_classes,\n",
    "                mode=model_mode,\n",
    "                l2_regularization=0.0005,\n",
    "                scales=[0.1, 0.2, 0.37, 0.54, 0.71, 0.88, 1.05], # The scales for MS COCO [0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05]\n",
    "                aspect_ratios_per_layer=[[1.0, 2.0, 0.5],\n",
    "                                         [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                                         [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                                         [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                                         [1.0, 2.0, 0.5],\n",
    "                                         [1.0, 2.0, 0.5]],\n",
    "                two_boxes_for_ar1=True,\n",
    "                steps=[8, 16, 32, 64, 100, 300],\n",
    "                offsets=[0.5, 0.5, 0.5, 0.5, 0.5, 0.5],\n",
    "                clip_boxes=False,\n",
    "                variances=[0.1, 0.1, 0.2, 0.2],\n",
    "                normalize_coords=True,\n",
    "                subtract_mean=[123, 117, 104],\n",
    "                swap_channels=[2, 1, 0],\n",
    "                confidence_thresh=0.01,\n",
    "                iou_threshold=0.45,\n",
    "                top_k=200,\n",
    "                nms_max_output_size=400)\n",
    "\n",
    "# 학습한 weight file의 경로를 입력합니다.\n",
    "weights_path = 'ssd300_pascal_07+12_epoch-04_loss-3.0387_val_loss-3.5244_weight.h5'\n",
    "\n",
    "model.load_weights(weights_path, by_name=True)\n",
    "\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "\n",
    "model.compile(optimizer=adam, loss=ssd_loss.compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C://Users//user/Desktop/SPARK/Concrete Crack Images for Classification/Positive 300 X 300/annotation'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-a0386d0dd209>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m                   \u001b[0mexclude_truncated\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                   \u001b[0mexclude_difficult\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m                   ret=False)\n\u001b[0m",
      "\u001b[1;32m~\\keras\\ssd_keras\\data_generator\\object_detection_2d_data_generator.py\u001b[0m in \u001b[0;36mparse_xml\u001b[1;34m(self, images_dirs, image_set_filenames, annotations_dirs, classes, include_classes, exclude_truncated, exclude_difficult, ret, verbose)\u001b[0m\n\u001b[0;32m    463\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mimages_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_set_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mannotations_dir\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages_dirs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_set_filenames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mannotations_dirs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m             \u001b[1;31m# Read the image set file that so that we know all the IDs of all the images to be included in the dataset.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 465\u001b[1;33m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_set_filename\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    466\u001b[0m                 \u001b[0mimage_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# Note: These are strings, not integers.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_ids\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mimage_ids\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'C://Users//user/Desktop/SPARK/Concrete Crack Images for Classification/Positive 300 X 300/annotation'"
     ]
    }
   ],
   "source": [
    "# 2. 성능 테스트를 위해 사용할 이미지들을 불러오고, 테스트 결과 값을 출력하는 부분입니다.\n",
    "\n",
    "dataset = DataGenerator()\n",
    "\n",
    "# Crack_images_dir = 균열 이미지들이 저장된 디렉토리 경로를 입력합니다.\n",
    "# Crack_images_dir_annotations = 균열 이미지들에 대한 Annotation 파일들이 저장된 디렉토리 경로를 입력합니다.\n",
    "# test_image_set_filename = 테스트에 사용할 파일명들을 모두 담은 텍스트 파일의 경로를 입력합니다.\n",
    "\n",
    "Crack_images_dir = 'C://Users//user/Desktop/SPARK/Concrete Crack Images for Classification/Positive 300 X 300/annotation/'\n",
    "Crack_images_dir_annotations = 'C://Users//user/Desktop/SPARK/Concrete Crack Images for Classification/Positive 300 X 300/annotation/'\n",
    "test_image_set_filename = 'C:\\\\Users\\\\user\\Desktop\\SPARK\\Concrete Crack Images for Classification\\Positive 300 X 300\\test.txt'\n",
    "\n",
    "classes = ['background','crack']\n",
    "\n",
    "dataset.parse_xml(images_dirs=[Crack_images_dir],\n",
    "                  image_set_filenames=[Crack_images_dir_annotations],\n",
    "                  annotations_dirs=[test_image_set_filename],\n",
    "                  classes=classes,\n",
    "                  include_classes='all',\n",
    "                  exclude_truncated=False,\n",
    "                  exclude_difficult=False,\n",
    "                  ret=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

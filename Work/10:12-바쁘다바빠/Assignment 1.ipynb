{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment\n",
    "The use of a GPU is not required for this notebook. Run the following cell to set up the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch nltk tqdm\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was tested with the following versions of python and torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python 3.9.13\n",
      "torch 1.12.1\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "import torch\n",
    "\n",
    "print(\"python\", python_version())\n",
    "print(\"torch\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Text Pre-processing (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This question will use a modified version of the data from the unreliable news dataset released by [Rashkin et al., 2017](https://aclanthology.org/D17-1317/). The dataset contains 15,000 news articles labeled as hoax, sattire, fake news etc. \n",
    "\n",
    "Start by loading the dataset with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def load_data(filename):\n",
    "    with open(filename) as f:\n",
    "        for line in csv.reader(f, delimiter=\"\\t\"):\n",
    "            yield line\n",
    "\n",
    "train_data = list(load_data(\"assignment1_data/train.tsv\"))\n",
    "test_data = list(load_data(\"assignment1_data/test.tsv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each item in `train_data` and `test_data` is a (2-tuple) of the label and sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12003\n",
      "2997\n",
      "\n",
      "['Satire', \"GREEN BAY, WIDavid Horsted, 45, announced Monday that he's seen a whole heck of a lot during his 20 years driving a taxi. 'Aw, geez, the people I've met and the places I've seenthe stories would make your head spin,' Horsted said. 'I've been from Lambeau Field to the Barhausen Waterfowl Preserve and every place in between. One time, one of the Packers even threw up in my cab, but I don't think I should say who.' With a little prodding, Horsted said the person's first name rhymes with 'baloney' and last name with 'sandwich.' \"]\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "print()\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1.1** (1 point) Use NLTK's `word_tokenize` to tokenize each document in the `train_data` and `test_data` and store these in two lists: `train_tokenized` and `test_tokenized`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Train Output:\n",
      "['GREEN', 'BAY', ',', 'WIDavid', 'Horsted', ',', '45', ',', 'announced', 'Monday', 'that', 'he', \"'s\", 'seen', 'a', 'whole', 'heck', 'of', 'a', 'lot', 'during', 'his', '20', 'years', 'driving', 'a', 'taxi', '.', \"'Aw\", ',', 'geez', ',', 'the', 'people', 'I', \"'ve\", 'met', 'and', 'the', 'places', 'I', \"'ve\", 'seenthe', 'stories', 'would', 'make', 'your', 'head', 'spin', ',', \"'\", 'Horsted', 'said', '.', \"'\", 'I', \"'ve\", 'been', 'from', 'Lambeau', 'Field', 'to', 'the', 'Barhausen', 'Waterfowl', 'Preserve', 'and', 'every', 'place', 'in', 'between', '.', 'One', 'time', ',', 'one', 'of', 'the', 'Packers', 'even', 'threw', 'up', 'in', 'my', 'cab', ',', 'but', 'I', 'do', \"n't\", 'think', 'I', 'should', 'say', 'who', '.', \"'\", 'With', 'a', 'little', 'prodding', ',', 'Horsted', 'said', 'the', 'person', \"'s\", 'first', 'name', 'rhymes', 'with', \"'baloney\", \"'\", 'and', 'last', 'name', 'with', \"'sandwich\", '.', \"'\"]\n",
      "\n",
      "Example Test Output:\n",
      "['The', 'Independents', 'Grill', 'NSA', 'Defender', 'On', 'Legality', 'Of', 'Surveillance', 'ProgramYoutube']\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE\n",
    "\n",
    "print(\"Example Train Output:\")\n",
    "print(train_tokenized[0])\n",
    "print()\n",
    "\n",
    "print(\"Example Test Output:\")\n",
    "print(test_tokenized[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1.2** (2 points) isntead of using NLTK's tokenizer, show splitting the string by whitespace for a small sample of instances. Discuss two limitations when string splitting with whitespace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limitation 1:\n",
    "\n",
    "\n",
    "\n",
    "Limitation 2:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1.3** (1 points) construct a vocabulary of all tokens in `train_tokenized`. Add an extra `UNK` token as a placeholder to account for unknown/unseen tokens at test time. How many unique tokens are present in this vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1.4** (2 points) We can reduce the size of the vocabulary by removing less frequently occuring words. Create a vocabulary for tokens in the which only contains tokens occuring at least (>=) 5 times. What is the size of the vocabulary now? (Remember to include the UNK placeholder token for unseen tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1.5** (1 point) suggest how additional pre-processing could be used to reduce the vocabulary size when tokenizing with NLTK's tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1.6** (1 point) Print the number of items in each class for the test dataset (`test_data`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Training a feed-forward network (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 2.1** (1 point) Create a dictionary of label names to a unique index and call this `label2idx`, create a dictionary of unique words from the vocabulary to an index and call this `word2idx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 2.2** (1 point) For each document in `train_tokenized` and `test_tokenized`, create a `torch.LongTensor` of the word IDs from `word2idx`. If a word does not appear in `word2idx`, replace it with a special token for unknwon values (`UNK`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 2.3** (3 points) Create a Multi Layer Perceptron to classify these documents that performs the following operations: \n",
    "\n",
    "* (1) uses `torch.Embedding` to create a $d$-dimensional continuous representation of each token (`embedding_size`), \n",
    "* (2) sums the word embeddings, \n",
    "* (3) uses `tanh` activation function, \n",
    "* (4) uses a torch.Linear layer to project to a hidden dimension (`hidden_size`), \n",
    "* (5) applies tanh activation to this hidden representation\n",
    "* (6) uses a linear layer to perform classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 20\n",
    "hidden_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 2.4** (3 points) train the model for 3 epochs and report the mean loss and the accuracy on the test set at each epoch. Use cross-entropy loss and the `Adam` optimizer (https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) with the learning rate set to `0.005`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Recurrent Neural Networks (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that a recurrent neural network conditionally encodes tokens given the previous hidden state $\\mathbf{h}_{t-1}$ and the input at the current input $\\mathbf{x}_t$:\n",
    "\n",
    "$$\n",
    "\\mathbf{h}_t = \\tanh (U\\mathbf{h}_{t-1} + V\\mathbf{x}_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 3.1** (1 point) Show that such recurrent neural network (RNN) without an activation function is equivalent to a single linear transformation with respect to the inputs, which means each $\\textbf{h}_t$ is a linear combination of the inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 3.2** (4 points) Provide your own implementation of an RNN cell (i.e. do NOT use the built in `torch.nn.RNN` method) that conforms to the following specification:\n",
    "\n",
    "* Input: a matrix of $N$ embeddings of dimension size $d$ describing a sequence of embeddings for tokens (matrix size: $\\mathbb{R}^{N \\times d}$)\n",
    "* Output: a tuple containing \n",
    "  * 1: A matrix containing the $N$ hidden states of dimension size $b$ (matrix size: $\\mathbb{R}^{N \\times b}$) \n",
    "  * 2: the final hidden state of the last element (vector of size $\\mathbb{R}^{b}$)\n",
    "  \n",
    "* Assume that the initial hidden state is a vector of zeros ($\\mathbf{h}_0 = [0,...,0]^T$)\n",
    "* Including bias term is optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_size, hidden_size):\n",
    "        super(RNNLayer, self).__init__()\n",
    "        \n",
    "        ## YOUR CODE HERE\n",
    "        \n",
    "        \n",
    "    def forward(self, input_embeddings): # input_embeddings size [num_words*embedding_size]  \n",
    "        \n",
    "        ## YOUR CODE HERE\n",
    "        \n",
    "        \n",
    "        # return a 2-tuple: (all hidden states, final hidden state)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 3.3** (1 point) create a copy of your model from problem 2.3 and change the summing of embeddings to instead use the final hidden state of your own RNN implementation. Use a copy of your training code from problem 2.4 and modify it to train your model on the first 100 items of the training set, reporting the mean loss and the accuracy on the first 100 items of the test set.\n",
    "\n",
    "*NOTE: Because training is slow, we limit the training and test data to 100 samples. There is no additional award for using all data*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 3.4** (2 points) A limitation of the RNN is the vanishing gradient and exploding gradient problem. Exploding gradients can be mitigated with gradient clipping. Describe the method and benefit of gradient clipping and provide a simple implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: LSTM (6 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 4.1** (2 points) Explain how the architecture of an LSTM can mitigate the vanishing gradient problem found in RNNs. (A complete proof is not necessary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 4.2** (4 points) Provide your own implementation of the LSTM (i.e. do NOT use the built in `torch.nn.LSTM` method) that conforms to the following specification:\n",
    "\n",
    "* Input: a matrix of $N$ embeddings of dimension size $d$ describing a sequence of embeddings for tokens (matrix size: $\\mathbb{R}^{N \\times d}$)\n",
    "* Output: a tuple containing \n",
    "  * 1: A matrix containing the $N$ hidden states of dimension size $b$ (matrix size: $\\mathbb{R}^{N \\times b}$) \n",
    "  * 2: the final hidden state of the last element (vector of size $\\mathbb{R}^{b}$)\n",
    "  \n",
    "* Assume that the initial hidden state is a vector of zeros ($\\mathbf{h}_0 = [0,...,0]^T$)\n",
    "* Assume that the initial cell state is a vector of zeros ($\\mathbf{c}_0 = [0,...,0]^T$)\n",
    "* Including bias term is optional\n",
    "\n",
    "\n",
    "Following problem 3.3, demonstrate training on the first 100 instances instances from the training set and report the accuracy and loss on the first 100 instances from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

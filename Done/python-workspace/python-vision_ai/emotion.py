# # -*- coding: utf-8 -*-
# """emotion.ipynb
#
# Automatically generated by Colaboratory.
#
# Original file is located at
#     https://colab.research.google.com/drive/1aDw-5G-vyl2CwH76EKEsvim5UVVxdqXH
# """
#
# !pip install --upgrade google-api-core google-cloud-vision
#
# pip install youtube-dl


import os
import io
import cv2
import dlib
import platform
import youtube_dl
import numpy as np
import matplotlib.pyplot as plt

from base64 import b64decode
from PIL import ImageFont, ImageDraw, Image
from IPython.display import Image, display, Javascript

from google.cloud import vision, drive
from google.colab.output import eval_js
from google.colab.patches import cv2_imshow

os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'service-account-file.json'

# Vision API Ŭ���̾�Ʈ ����
client_options = {'api_endpoint': 'eu-vision.googleapis.com'}
client = vision.ImageAnnotatorClient(client_options=client_options)

drive.mount('/content/drive')


def get_video_duration(url):
    ydl_opts = {'quiet': True, 'force_generic_extractor': True, 'force_duration': True}
    with youtube_dl.YoutubeDL(ydl_opts) as ydl:
        info_dict = ydl.extract_info(url, download=False)
        duration = info_dict.get('duration')
        return duration


# ��� ����
url = 'https://www.youtube.com/watch?v=yMESq858oHg&pp=ygURMeu2hOynnOumrCDrhbjrnpg%3D'
duration = get_video_duration(url)

if duration:
    print(f"The video duration is {duration} seconds.")
else:
    print("Failed to retrieve video duration.")


def take_photos(filename='photo.jpg', quality=0.8, interval=3, total_time=duration):
    js = Javascript('''
      async function takePhotos(quality, interval, total_time) {
        const div = document.createElement('div');
        const video = document.createElement('video');
        video.style.display = 'block';
        const stream = await navigator.mediaDevices.getUserMedia({video: true});

        document.body.appendChild(div);
        div.appendChild(video);
        video.srcObject = stream;
        await video.play();

        const photos = [];
        let index = 1;  // �̹��� �ε���

        const start_time = Date.now();

        while (Date.now() - start_time < total_time * 1000) {
          const canvas = document.createElement('canvas');
          canvas.width = video.videoWidth;
          canvas.height = video.videoHeight;
          canvas.getContext('2d').drawImage(video, 0, 0);
          const photoData = canvas.toDataURL('image/jpeg', quality);
          photos.push({data: photoData, index: index});  // �����Ϳ� �ε����� ��ü�� ����
          index += 1;
          await new Promise(resolve => setTimeout(resolve, interval * 1000));
        }

        stream.getVideoTracks()[0].stop();
        div.remove();
        return photos;
      }
      ''')
    display(js)
    photos_data = eval_js('takePhotos({}, {}, {})'.format(quality, interval, total_time))

    # ����� ���� ���� ���
    filenames = []

    for i, photo_data in enumerate(photos_data):
        binary = b64decode(photo_data['data'].split(',')[1])
        filename_i = f"{filename.split('.')[0]}_{photo_data['index']}.{filename.split('.')[1]}"
        filenames.append(filename_i)
        with open(filename_i, 'wb') as f:
            f.write(binary)

    return filenames


# Dlib�� �� ������ �ʱ�ȭ
detector = dlib.get_frontal_face_detector()

# �Ʒõ� ���� �з� �� �ҷ����� (���÷� 'haarcascade_smile.xml' ���)
emotion_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_smile.xml')


def detect_emotions(file_name):
    # �̹��� �б�
    img = cv2.imread(file_name)

    # �̹����� �ùٸ��� �ε�Ǿ����� Ȯ��
    if img is None:
        print(f"Failed to load image: {file_name}")
        return

    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    # �� ����
    faces = detector(gray)

    for face in faces:
        x, y, w, h = (face.left(), face.top(), face.width(), face.height())

        # �� ������ �簢�� �׸���
        cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)

        # ���� �з�
        roi_gray = gray[y:y + h, x:x + w]
        smiles = emotion_cascade.detectMultiScale(roi_gray, scaleFactor=1.8, minNeighbors=20)

        for (sx, sy, sw, sh) in smiles:
            cv2.rectangle(img, (x + sx, y + sy), (x + sx + sw, y + sy + sh), (0, 255, 0), 2)

    cv2_imshow(img)


def extract_emotions(path):
    client = vision.ImageAnnotatorClient()

    with open(path, "rb") as image_file:
        content = image_file.read()

    image = vision.Image(content=content)

    response = client.face_detection(image=image)
    faces = response.face_annotations

    emotions = []

    likelihood_name = (
        "UNKNOWN",
        "VERY_UNLIKELY",
        "UNLIKELY",
        "POSSIBLE",
        "LIKELY",
        "VERY_LIKELY",
    )

    for face in faces:
        sorrow_likelihood = likelihood_name[face.sorrow_likelihood]
        anger_likelihood = likelihood_name[face.anger_likelihood]
        joy_likelihood = likelihood_name[face.joy_likelihood]
        surprise_likelihood = likelihood_name[face.surprise_likelihood]

        # ��� ������ ���ɼ��� 'UNKNOWN'�̰ų� 'VERY_UNLIKELY'�� �� 'no sentiment'�� �Ǵ�
        if all(likeli in ['UNKNOWN', 'VERY_UNLIKELY'] for likeli in [
            sorrow_likelihood,
            anger_likelihood,
            joy_likelihood,
            surprise_likelihood
        ]):
            face_emotion = 'no sentiment'
        else:
            # ���� ���ɼ��� ���� ���� ������ Ž��
            emotions_likelihood = [sorrow_likelihood, anger_likelihood, joy_likelihood, surprise_likelihood]
            max_likelihood = max(emotions_likelihood, key=lambda likeli: likelihood_name.index(likeli))
            if max_likelihood == 'UNKNOWN' or max_likelihood == 'VERY_UNLIKELY':
                face_emotion = 'no sentiment'
            else:
                # ���� ���ɼ��� ���� ������ �̸����� ����
                face_emotion = next(
                    (emo for emo, likeli in zip(['sorrow', 'anger', 'joy', 'surprise'], emotions_likelihood) if
                     likeli == max_likelihood), 'no sentiment')

        emotions.append({
            "sorrow": sorrow_likelihood,
            "anger": anger_likelihood,
            "joy": joy_likelihood,
            "surprise": surprise_likelihood,
            "overall_emotion": face_emotion  # �߰��� ��ü ���� ����
        })

    return emotions


try:
    filenames = take_photos()
    for filename in filenames:
        print('Saved to {}'.format(filename))
        display(Image(filename))
except Exception as err:
    print(str(err))

####################################
import pymysql

host = 'localhost'
user = 'root'
pwd = 'root'


def execute(sql):
    conn = pymysql.connect(host=host, user=user, password=pwd, charset='utf8')
    cursor = conn.cursor()
    cursor.execute(sql)
    conn.commit()
    conn.close()


init_table = '''
CREATE TABLE EmotionLog (
    id INT AUTO_INCREMENT PRIMARY KEY,
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    overall_emotion VARCHAR(50) NOT NULL,
    sorrow_likelihood INT,
    anger_likelihood INT,
    joy_likelihood INT,
    surprise_likelihood INT
);
'''

####################################

if __name__ == '__main__':
    execute(init_table)
    for file_name in filenames:
        print(f"Running on {file_name}...")
        detect_emotions(file_name)
        extracted_emotions = extract_emotions(file_name)

        for idx, emotion in enumerate(extracted_emotions):
            print(f"Emotions for face {idx + 1}:")
            print(f"Sorrow: {emotion['sorrow']}")
            print(f"Anger: {emotion['anger']}")
            print(f"Joy: {emotion['joy']}")
            print(f"Surprise: {emotion['surprise']}")
            print("\n")

            overall = max(emotion['sorrow'], emotion['anger'], emotion['joy'], emotion['surprise'])

            if overall == emotion['sorrow']:
                overall = 'sorrow'
            elif overall == emotion['anger']:
                overall = 'anger'
            elif overall == emotion['joy']:
                overall = 'joy'
            elif overall == emotion['surprise']:
                overall = 'surprise'

            sql = f'''
            INSERT INTO EmotionLog (
            overall_emotion,
            sorrow_likeihood,
            anger_likeihood,
            joy_likeihood,
            surprise_likeihood,
            VALUES ({overall}, {emotion['sorrow']}, {emotion['anger']}, {emotion['joy']}, {emotion['surprise']})
            '''

            execute(sql)

        print(f"Finished {file_name}\n")

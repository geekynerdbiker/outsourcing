{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "aztORJ_82ORf"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 회귀란 데이터를 통한 다음 데이터를 예측하기 위해 사용하는 기법입니다. 데이터셋은 통계청에서 사망원인통계 자료를 이용해서 남성의 사망률을 예측합니다.\n",
        "\n",
        "링크\n",
        "https://kosis.kr/statHtml/statHtml.do?orgId=101&tblId=DT_1B34E07&vw_cd=MT_ZTITLE&list_id=F_27&scrId=&seqNo=&lang_mode=ko&obj_var_id=&itm_id=&conn_path=MT_ZTITLE&path=%252FstatisticsList%252FstatisticsListIndex.do\n",
        "\n",
        "여기 입니다.\n"
      ],
      "metadata": {
        "id": "3lH7NGGd6sRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# csv 파일 읽기\n",
        "df = pd.read_csv(\"data.csv\")"
      ],
      "metadata": {
        "id": "vkK0Pl7q2P4G"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler \n",
        "\n",
        "# 연도와 남성 10만명당 사망률을 numpy array로 생성\n",
        "year = np.array(df[[\"시점\"]]).astype(float)\n",
        "men = np.array(df[[\"남자\"]]).astype(float)\n",
        "\n",
        "# 범위 학습\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(year)\n",
        "\n",
        "# 학습된 범위로 변환\n",
        "trans_year = scaler.transform(year)\n",
        "trans_men = scaler.transform(men)\n",
        "\n",
        "# loss 비교를 위한 배열\n",
        "linear_loss = []\n",
        "deep_loss = []"
      ],
      "metadata": {
        "id": "huO7qs1p82ad"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 통계 자료 그래프로 출력\n",
        "plt.plot(year, men, 'o')\n",
        "plt.title(\"Death Rate of Men per Year\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "4a0ScOxL2RKI",
        "outputId": "7ca34df2-3058-4315-b37c-8bfa1192b5f4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Linear Regression Model"
      ],
      "metadata": {
        "id": "jcNuCqL6FOIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear Regression 실시\n",
        "\n",
        "W = 0.0\n",
        "b = 0.0\n",
        "\n",
        "year_data = len(year)\n",
        "\n",
        "men_data = len(men)\n",
        "\n",
        "# 반복횟수와 학습률 설정\n",
        "epochs = 5000\n",
        "learning_rate = 0.001\n",
        "\n",
        "for i in range(epochs):\n",
        "    hypothesis = trans_year * W + b\n",
        "    # MSE 계산\n",
        "    loss = np.sum((hypothesis - trans_men) ** 2) / year_data\n",
        "    # 목적 함수인 W에 대하여 편미분\n",
        "    gradient_w = np.sum((W * trans_year - trans_men + b) * 2 * trans_year) / year_data\n",
        "    # b에 대하여 편미분\n",
        "    gradient_b = np.sum((W * trans_year - trans_men + b) * 2) / year_data\n",
        "\n",
        "    # W, b 업데이트\n",
        "    W -= learning_rate * gradient_w\n",
        "    b -= learning_rate * gradient_b\n",
        "\n",
        "    # 100번째 반복마다 loss, W, b 출력\n",
        "    if i % 100 == 0:\n",
        "        print('Epoch ({:10d}/{:10d}) loss: {:10f}, W: {:10f}, b:{:10f}'.format(i, epochs, loss, W, b))\n",
        "        linear_loss.append(loss)\n",
        "\n",
        "print('W: {:10f}'.format(W))\n",
        "print('b: {:10f}'.format(b))\n",
        "print('result : ')\n",
        "print(year * W + b)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvE6SPiE9XbF",
        "outputId": "9a446fd2-1ffd-4de4-b301-76b905d284de"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch (         0/      5000) loss: 16273.176855, W:  -0.004482, b: -0.255006\n",
            "Epoch (       100/      5000) loss: 10907.548861, W:  -0.410227, b:-23.342006\n",
            "Epoch (       200/      5000) loss: 7312.301165, W:  -0.742356, b:-42.240258\n",
            "Epoch (       300/      5000) loss: 4903.299644, W:  -1.014226, b:-57.709740\n",
            "Epoch (       400/      5000) loss: 3289.144288, W:  -1.236770, b:-70.372544\n",
            "Epoch (       500/      5000) loss: 2207.576886, W:  -1.418937, b:-80.737895\n",
            "Epoch (       600/      5000) loss: 1482.870904, W:  -1.568053, b:-89.222628\n",
            "Epoch (       700/      5000) loss: 997.280490, W:  -1.690114, b:-96.167948\n",
            "Epoch (       800/      5000) loss: 671.909851, W:  -1.790029, b:-101.853157\n",
            "Epoch (       900/      5000) loss: 453.894728, W:  -1.871817, b:-106.506880\n",
            "Epoch (      1000/      5000) loss: 307.813344, W:  -1.938765, b:-110.316263\n",
            "Epoch (      1100/      5000) loss: 209.931276, W:  -1.993567, b:-113.434498\n",
            "Epoch (      1200/      5000) loss: 144.345239, W:  -2.038426, b:-115.986981\n",
            "Epoch (      1300/      5000) loss: 100.399209, W:  -2.075146, b:-118.076359\n",
            "Epoch (      1400/      5000) loss:  70.953100, W:  -2.105203, b:-119.786655\n",
            "Epoch (      1500/      5000) loss:  51.222688, W:  -2.129808, b:-121.186646\n",
            "Epoch (      1600/      5000) loss:  38.002293, W:  -2.149948, b:-122.332632\n",
            "Epoch (      1700/      5000) loss:  29.143946, W:  -2.166434, b:-123.270699\n",
            "Epoch (      1800/      5000) loss:  23.208397, W:  -2.179929, b:-124.038569\n",
            "Epoch (      1900/      5000) loss:  19.231272, W:  -2.190976, b:-124.667121\n",
            "Epoch (      2000/      5000) loss:  16.566394, W:  -2.200018, b:-125.181634\n",
            "Epoch (      2100/      5000) loss:  14.780787, W:  -2.207420, b:-125.602797\n",
            "Epoch (      2200/      5000) loss:  13.584339, W:  -2.213479, b:-125.947547\n",
            "Epoch (      2300/      5000) loss:  12.782657, W:  -2.218438, b:-126.229748\n",
            "Epoch (      2400/      5000) loss:  12.245488, W:  -2.222498, b:-126.460748\n",
            "Epoch (      2500/      5000) loss:  11.885558, W:  -2.225821, b:-126.649837\n",
            "Epoch (      2600/      5000) loss:  11.644386, W:  -2.228541, b:-126.804619\n",
            "Epoch (      2700/      5000) loss:  11.482788, W:  -2.230768, b:-126.931319\n",
            "Epoch (      2800/      5000) loss:  11.374509, W:  -2.232591, b:-127.035031\n",
            "Epoch (      2900/      5000) loss:  11.301957, W:  -2.234083, b:-127.119926\n",
            "Epoch (      3000/      5000) loss:  11.253343, W:  -2.235304, b:-127.189418\n",
            "Epoch (      3100/      5000) loss:  11.220769, W:  -2.236304, b:-127.246302\n",
            "Epoch (      3200/      5000) loss:  11.198943, W:  -2.237122, b:-127.292866\n",
            "Epoch (      3300/      5000) loss:  11.184318, W:  -2.237792, b:-127.330981\n",
            "Epoch (      3400/      5000) loss:  11.174519, W:  -2.238340, b:-127.362181\n",
            "Epoch (      3500/      5000) loss:  11.167953, W:  -2.238789, b:-127.387720\n",
            "Epoch (      3600/      5000) loss:  11.163553, W:  -2.239156, b:-127.408626\n",
            "Epoch (      3700/      5000) loss:  11.160606, W:  -2.239457, b:-127.425739\n",
            "Epoch (      3800/      5000) loss:  11.158630, W:  -2.239703, b:-127.439746\n",
            "Epoch (      3900/      5000) loss:  11.157307, W:  -2.239905, b:-127.451213\n",
            "Epoch (      4000/      5000) loss:  11.156420, W:  -2.240070, b:-127.460599\n",
            "Epoch (      4100/      5000) loss:  11.155826, W:  -2.240205, b:-127.468282\n",
            "Epoch (      4200/      5000) loss:  11.155428, W:  -2.240315, b:-127.474571\n",
            "Epoch (      4300/      5000) loss:  11.155161, W:  -2.240406, b:-127.479719\n",
            "Epoch (      4400/      5000) loss:  11.154982, W:  -2.240480, b:-127.483933\n",
            "Epoch (      4500/      5000) loss:  11.154862, W:  -2.240541, b:-127.487382\n",
            "Epoch (      4600/      5000) loss:  11.154782, W:  -2.240590, b:-127.490206\n",
            "Epoch (      4700/      5000) loss:  11.154728, W:  -2.240631, b:-127.492517\n",
            "Epoch (      4800/      5000) loss:  11.154692, W:  -2.240664, b:-127.494409\n",
            "Epoch (      4900/      5000) loss:  11.154668, W:  -2.240691, b:-127.495958\n",
            "W:  -2.240713\n",
            "b: -127.497214\n",
            "result : \n",
            "[[-4570.8318352 ]\n",
            " [-4573.07254857]\n",
            " [-4575.31326195]\n",
            " [-4577.55397532]\n",
            " [-4579.7946887 ]\n",
            " [-4582.03540207]\n",
            " [-4584.27611544]\n",
            " [-4586.51682882]\n",
            " [-4588.75754219]\n",
            " [-4590.99825557]\n",
            " [-4593.23896894]\n",
            " [-4595.47968232]\n",
            " [-4597.72039569]\n",
            " [-4599.96110906]\n",
            " [-4602.20182244]\n",
            " [-4604.44253581]\n",
            " [-4606.68324919]\n",
            " [-4608.92396256]\n",
            " [-4611.16467594]\n",
            " [-4613.40538931]\n",
            " [-4615.64610268]\n",
            " [-4617.88681606]\n",
            " [-4620.12752943]\n",
            " [-4622.36824281]\n",
            " [-4624.60895618]\n",
            " [-4626.84966956]\n",
            " [-4629.09038293]\n",
            " [-4631.3310963 ]\n",
            " [-4633.57180968]\n",
            " [-4635.81252305]\n",
            " [-4638.05323643]\n",
            " [-4640.2939498 ]\n",
            " [-4642.53466318]\n",
            " [-4644.77537655]\n",
            " [-4647.01608992]\n",
            " [-4649.2568033 ]\n",
            " [-4651.49751667]\n",
            " [-4653.73823005]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Deep Learning 기반의 Regression"
      ],
      "metadata": {
        "id": "E-mvnqIVFEGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Deep Learning Regression 실시\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "\n",
        "# 데이터 셋을 훈련 데이터와 테스트 데이터로 분할\n",
        "dataset = df\n",
        "train_dataset = df.sample(frac=0.8,random_state=0)\n",
        "test_dataset = df.drop(train_dataset.index)"
      ],
      "metadata": {
        "id": "UXXz94y42SXN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 통계 확인용\n",
        "train_stats_men = train_dataset.describe()\n",
        "train_stats_men.pop(\"남자\")\n",
        "train_stats_men = train_stats_men.transpose()"
      ],
      "metadata": {
        "id": "VhAN98yqKUd8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 특성에서 레이블 분리\n",
        "train_labels_men = train_dataset.pop('남자')\n",
        "test_labels_men = test_dataset.pop('남자')"
      ],
      "metadata": {
        "id": "3AbVvdYvGfvK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 정규화\n",
        "def norm_men(x):\n",
        "  return (x - train_stats_men['mean']) / train_stats_men['std']\n",
        "\n",
        "normed_train_data_men = norm_men(train_dataset)\n",
        "normed_test_data_men = norm_men(test_dataset)"
      ],
      "metadata": {
        "id": "I8YQ0L8wGWij"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 시퀀스 모델 생성 함수\n",
        "def build_model():\n",
        "  model = keras.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=[len(train_dataset.keys())]),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "  optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
        "\n",
        "  model.compile(loss='mse',\n",
        "                optimizer=optimizer,\n",
        "                metrics=['mae', 'mse'])\n",
        "  return model"
      ],
      "metadata": {
        "id": "o5G2PH_yI9c1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 생성\n",
        "model = build_model()"
      ],
      "metadata": {
        "id": "JifiGhW-I9_v"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 반복 횟수 설정\n",
        "EPOCHS = 5000\n",
        "\n",
        "# 모델 훈련\n",
        "history_men = model.fit(\n",
        "  normed_train_data_men, train_labels_men,\n",
        "  epochs=EPOCHS, validation_split = 0.2, verbose=0)"
      ],
      "metadata": {
        "id": "SWSsHay0JMdy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 결과 예측\n",
        "example_batch = normed_train_data_men[:10]\n",
        "example_result = model.predict(example_batch)\n",
        "example_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1BW2lOdJcIK",
        "outputId": "d608fe44-6d8b-46aa-82ca-bd6a8574f92c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[553.962  ],\n",
              "       [578.25   ],\n",
              "       [584.71136],\n",
              "       [584.6802 ],\n",
              "       [603.3247 ],\n",
              "       [563.80945],\n",
              "       [557.8717 ],\n",
              "       [597.6959 ],\n",
              "       [554.3234 ],\n",
              "       [685.78174]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 훈련과정 시각화\n",
        "h = pd.DataFrame(history_men.history)\n",
        "h['epoch'] = history_men.epoch\n",
        "\n",
        "for i in range(5000):\n",
        "  if i % 100 == 0:\n",
        "    deep_loss.append(h.loc[i]['loss'])"
      ],
      "metadata": {
        "id": "4M8pO4HEMuDu"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. linear regression과 deep learning 기반의 regression의 loss 값 비교"
      ],
      "metadata": {
        "id": "r9oRrSs4OKjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(deep_loss)):\n",
        "  print('['+str(i)+']' + ' linear regression loss: ' + str(linear_loss[i]))\n",
        "  print('['+str(i)+']' + ' deep learning regression loss: ' + str(deep_loss[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEJmfFXSOPaa",
        "outputId": "ca44e838-f6dd-4a5b-a41b-f559556ad9db"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0] linear regression loss: 16273.176855235804\n",
            "[0] deep learning regression loss: 357887.28125\n",
            "[1] linear regression loss: 10907.548861016361\n",
            "[1] deep learning regression loss: 338213.625\n",
            "[2] linear regression loss: 7312.301164772789\n",
            "[2] deep learning regression loss: 286083.21875\n",
            "[3] linear regression loss: 4903.299644117594\n",
            "[3] deep learning regression loss: 202291.921875\n",
            "[4] linear regression loss: 3289.1442877057125\n",
            "[4] deep learning regression loss: 110372.2109375\n",
            "[5] linear regression loss: 2207.576886318382\n",
            "[5] deep learning regression loss: 39780.7109375\n",
            "[6] linear regression loss: 1482.8709036525775\n",
            "[6] deep learning regression loss: 7393.64697265625\n",
            "[7] linear regression loss: 997.2804904819\n",
            "[7] deep learning regression loss: 1362.0521240234375\n",
            "[8] linear regression loss: 671.9098505212721\n",
            "[8] deep learning regression loss: 352.8802185058594\n",
            "[9] linear regression loss: 453.89472815274235\n",
            "[9] deep learning regression loss: 150.68960571289062\n",
            "[10] linear regression loss: 307.8133435904619\n",
            "[10] deep learning regression loss: 77.57792663574219\n",
            "[11] linear regression loss: 209.93127612744834\n",
            "[11] deep learning regression loss: 45.08806228637695\n",
            "[12] linear regression loss: 144.34523886786263\n",
            "[12] deep learning regression loss: 34.089847564697266\n",
            "[13] linear regression loss: 100.39920876340861\n",
            "[13] deep learning regression loss: 29.5821590423584\n",
            "[14] linear regression loss: 70.95310037454712\n",
            "[14] deep learning regression loss: 25.951553344726562\n",
            "[15] linear regression loss: 51.222687930293084\n",
            "[15] deep learning regression loss: 22.849943161010742\n",
            "[16] linear regression loss: 38.002293232307345\n",
            "[16] deep learning regression loss: 19.430652618408203\n",
            "[17] linear regression loss: 29.143946430671242\n",
            "[17] deep learning regression loss: 17.95649528503418\n",
            "[18] linear regression loss: 23.208396861185346\n",
            "[18] deep learning regression loss: 16.37203025817871\n",
            "[19] linear regression loss: 19.231272293730644\n",
            "[19] deep learning regression loss: 15.580917358398438\n",
            "[20] linear regression loss: 16.566393559271333\n",
            "[20] deep learning regression loss: 15.144242286682129\n",
            "[21] linear regression loss: 14.780787262831083\n",
            "[21] deep learning regression loss: 14.734847068786621\n",
            "[22] linear regression loss: 13.584338882400917\n",
            "[22] deep learning regression loss: 14.331833839416504\n",
            "[23] linear regression loss: 12.782656714339694\n",
            "[23] deep learning regression loss: 14.034022331237793\n",
            "[24] linear regression loss: 12.245488283925441\n",
            "[24] deep learning regression loss: 13.624371528625488\n",
            "[25] linear regression loss: 11.885557710277364\n",
            "[25] deep learning regression loss: 13.351333618164062\n",
            "[26] linear regression loss: 11.644385648570998\n",
            "[26] deep learning regression loss: 13.108650207519531\n",
            "[27] linear regression loss: 11.482787919436067\n",
            "[27] deep learning regression loss: 12.852127075195312\n",
            "[28] linear regression loss: 11.374509100252903\n",
            "[28] deep learning regression loss: 12.650813102722168\n",
            "[29] linear regression loss: 11.301956702725532\n",
            "[29] deep learning regression loss: 12.444664001464844\n",
            "[30] linear regression loss: 11.253342851681865\n",
            "[30] deep learning regression loss: 12.241612434387207\n",
            "[31] linear regression loss: 11.220769062340006\n",
            "[31] deep learning regression loss: 12.046332359313965\n",
            "[32] linear regression loss: 11.198942942225958\n",
            "[32] deep learning regression loss: 11.859073638916016\n",
            "[33] linear regression loss: 11.184318315221875\n",
            "[33] deep learning regression loss: 11.671473503112793\n",
            "[34] linear regression loss: 11.174519060297486\n",
            "[34] deep learning regression loss: 11.496541023254395\n",
            "[35] linear regression loss: 11.167953053721954\n",
            "[35] deep learning regression loss: 11.344927787780762\n",
            "[36] linear regression loss: 11.163553490420208\n",
            "[36] deep learning regression loss: 11.181353569030762\n",
            "[37] linear regression loss: 11.160605555930143\n",
            "[37] deep learning regression loss: 11.032017707824707\n",
            "[38] linear regression loss: 11.15863028766786\n",
            "[38] deep learning regression loss: 10.888007164001465\n",
            "[39] linear regression loss: 11.15730675598117\n",
            "[39] deep learning regression loss: 10.738929748535156\n",
            "[40] linear regression loss: 11.156419921438665\n",
            "[40] deep learning regression loss: 10.608992576599121\n",
            "[41] linear regression loss: 11.155825696522342\n",
            "[41] deep learning regression loss: 10.474535942077637\n",
            "[42] linear regression loss: 11.15542753515823\n",
            "[42] deep learning regression loss: 10.356939315795898\n",
            "[43] linear regression loss: 11.155160746493683\n",
            "[43] deep learning regression loss: 10.235762596130371\n",
            "[44] linear regression loss: 11.15498198431849\n",
            "[44] deep learning regression loss: 10.117892265319824\n",
            "[45] linear regression loss: 11.154862204434508\n",
            "[45] deep learning regression loss: 10.000184059143066\n",
            "[46] linear regression loss: 11.154781945729956\n",
            "[46] deep learning regression loss: 9.900008201599121\n",
            "[47] linear regression loss: 11.154728168255462\n",
            "[47] deep learning regression loss: 9.780871391296387\n",
            "[48] linear regression loss: 11.154692134571874\n",
            "[48] deep learning regression loss: 9.684823036193848\n",
            "[49] linear regression loss: 11.15466799014407\n",
            "[49] deep learning regression loss: 9.588845252990723\n"
          ]
        }
      ]
    }
  ]
}
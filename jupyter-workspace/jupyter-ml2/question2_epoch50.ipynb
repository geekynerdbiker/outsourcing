{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0NSl3gi88cWe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "outputId": "da3b4d08-8fc6-4318-dc6f-d8805c688d08"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pytorch_fid'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e66d6f9b50ff>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_fid\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfid_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mssl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mssl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_default_https_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mssl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_unverified_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_fid'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, utils\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.autograd as autograd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from pytorch_fid import fid_score\n",
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkGtxGBR8cWf"
      },
      "source": [
        "### 1. Data Preparation & Data Preprocessing:    - Score: 10 points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUHLuLDe8cWg"
      },
      "outputs": [],
      "source": [
        "pth_to_imgs = \"img_align_celeba\"\n",
        "imgs = glob.glob(os.path.join(pth_to_imgs, \"*\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z62yUWwX8cWg"
      },
      "outputs": [],
      "source": [
        "class CelebADataset(Dataset):\n",
        "    def __init__(self, img_paths, transform=None):\n",
        "        self.img_paths = img_paths\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_paths[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFaD72AJ8cWg"
      },
      "source": [
        "### 이미지 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fgtSsTJ8cWg"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTTH3K2U8cWg"
      },
      "outputs": [],
      "source": [
        "dataset = CelebADataset(imgs, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "images = next(data_iter)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "for i in range(16):\n",
        "    plt.subplot(4, 4, i + 1)\n",
        "    plt.imshow(((images[i].permute(1, 2, 0) + 1) / 2).numpy())\n",
        "    plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQm6DXBb8cWh"
      },
      "source": [
        "### 2. Generator and Discriminator: - Score: 20 points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-oJemfA8cWh"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, z_dim=100):\n",
        "        super(Generator, self).__init__()\n",
        "        self.init_size = 4\n",
        "        self.l1 = nn.Sequential(nn.Linear(z_dim, 256 * self.init_size * self.init_size))\n",
        "\n",
        "        self.conv_blocks = nn.Sequential(\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(64, 32, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(32, 3, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        out = self.l1(z)\n",
        "        out = out.view(out.shape[0], 256, self.init_size, self.init_size)\n",
        "        img = self.conv_blocks(out)\n",
        "        return img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdWXCHow8cWh"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.utils.spectral_norm(nn.Conv2d(3, 32, 4, 2, 1, bias=False)),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.utils.spectral_norm(nn.Conv2d(32, 64, 4, 2, 1, bias=False)),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.utils.spectral_norm(nn.Conv2d(64, 128, 4, 2, 1, bias=False)),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.utils.spectral_norm(nn.Conv2d(128, 256, 4, 2, 1, bias=False)),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(256, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.model(x)\n",
        "        return out.view(-1, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgm2iJ-C8cWh"
      },
      "outputs": [],
      "source": [
        "def gradient_penalty(D, real_data, fake_data, device):\n",
        "    alpha = torch.rand(real_data.size(0), 1, 1, 1, device=device)\n",
        "    interpolates = (alpha * real_data + (1 - alpha) * fake_data).requires_grad_(True)\n",
        "    d_interpolates = D(interpolates)\n",
        "    gradients = autograd.grad(\n",
        "        outputs=d_interpolates,\n",
        "        inputs=interpolates,\n",
        "        grad_outputs=torch.ones_like(d_interpolates),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        only_inputs=True\n",
        "    )[0]\n",
        "    gradients = gradients.view(gradients.size(0), -1)\n",
        "    return ((gradients.norm(2, dim=1) - 1) ** 2).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95_sk9X48cWh"
      },
      "source": [
        "### 3. Training: - Score: 10 points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdWYDklb8cWi"
      },
      "outputs": [],
      "source": [
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "z_dim = 100\n",
        "G = Generator(z_dim).to(device)\n",
        "D = Discriminator().to(device)\n",
        "\n",
        "if os.path.isfile(\"saved_models/generator.pth\") :\n",
        "    G.load_state_dict(torch.load('saved_models/generator.pth', map_location=device))\n",
        "    D.load_state_dict(torch.load('saved_models/discriminator.pth', map_location=device))\n",
        "    G.eval()\n",
        "    D.eval()\n",
        "    print(\"Generator와 Discriminator 기존 모델 불러오기\")\n",
        "\n",
        "else :\n",
        "    optimizer_G = optim.Adam(G.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
        "    optimizer_D = optim.Adam(D.parameters(), lr=0.00005, betas=(0.5, 0.999))\n",
        "    lambda_gp = 10\n",
        "    num_epochs = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPXMpoei8cWi"
      },
      "outputs": [],
      "source": [
        "if not os.path.isfile(\"saved_models/generator.pth\"):\n",
        "    os.makedirs('saved_models', exist_ok=True)\n",
        "    os.makedirs('training_logs', exist_ok=True)\n",
        "    os.makedirs('generated_samples', exist_ok=True)\n",
        "\n",
        "    d_loss_values, g_loss_values = [], []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, real_imgs in enumerate(dataloader):\n",
        "            real_imgs = real_imgs.to(device)\n",
        "            batch_size = real_imgs.size(0)\n",
        "\n",
        "            z = torch.randn(batch_size, z_dim, 1, 1, device=device)\n",
        "            fake_imgs = G(z).detach()\n",
        "            real_validity = D(real_imgs)\n",
        "            fake_validity = D(fake_imgs)\n",
        "            gp = gradient_penalty(D, real_imgs, fake_imgs, device)\n",
        "\n",
        "            d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gp\n",
        "            optimizer_D.zero_grad()\n",
        "            d_loss.backward(retain_graph=True)\n",
        "            optimizer_D.step()\n",
        "\n",
        "            if i % 5 == 0:\n",
        "                z = torch.randn(batch_size, z_dim, 1, 1, device=device)\n",
        "                fake_imgs = G(z)\n",
        "                fake_validity = D(fake_imgs)\n",
        "                g_loss = -torch.mean(fake_validity)\n",
        "                optimizer_G.zero_grad()\n",
        "                g_loss.backward()\n",
        "                optimizer_G.step()\n",
        "\n",
        "            d_loss_values.append(d_loss.item())\n",
        "            g_loss_values.append(g_loss.item())\n",
        "\n",
        "            if i % 50 == 0:\n",
        "                with torch.no_grad():\n",
        "                    sample_z = torch.randn(16, z_dim, 1, 1, device=device)\n",
        "                    generated_imgs = G(sample_z).cpu()\n",
        "                    grid = utils.make_grid(generated_imgs, nrow=4, normalize=True)\n",
        "                    utils.save_image(grid, f'generated_samples/epoch_{epoch+1}_step_{i}.png')\n",
        "\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i}/{len(dataloader)}], \"\n",
        "                      f\"D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n",
        "        torch.save(G.state_dict(), f\"saved_models/generator_epoch_{epoch+1}.pth\")\n",
        "        torch.save(D.state_dict(), f\"saved_models/discriminator_epoch_{epoch+1}.pth\")\n",
        "        print(f\"Models saved for epoch {epoch+1}\")\n",
        "\n",
        "    torch.save(G.state_dict(), f\"saved_models/generator.pth\")\n",
        "    torch.save(D.state_dict(), f\"saved_models/discriminator.pth\")\n",
        "    print(f\"Models saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKuYOafS8cWi"
      },
      "source": [
        "### 4. Evaluation: - Score: 10 points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKVdICZv8cWi"
      },
      "outputs": [],
      "source": [
        "def save_real_images(dataloader, num_samples=1000, save_dir='real_images_subset'):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    count = 0\n",
        "    for imgs in dataloader:\n",
        "        for img in imgs:\n",
        "            img = ((img + 1) / 2).clamp(0, 1)\n",
        "            utils.save_image(img, os.path.join(save_dir, f\"real_{count}.png\"))\n",
        "            count += 1\n",
        "            if count >= num_samples:\n",
        "                return\n",
        "\n",
        "os.makedirs('real_images_subset', exist_ok=True)\n",
        "if len(os.listdir('real_images_subset')) < 1000:\n",
        "    save_real_images(dataloader, num_samples=1000, save_dir='real_images_subset')\n",
        "    print(\"Saved 1000 real images for FID calculation.\")\n",
        "else:\n",
        "    print(\"Many Images.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjCN1lIN8cWi"
      },
      "outputs": [],
      "source": [
        "def save_generated_images(G, z_dim=100, num_images=1000, batch_size=64, save_dir='generated_images_fid'):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    G.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, num_images, batch_size):\n",
        "            current_batch_size = min(batch_size, num_images - i)\n",
        "            z = torch.randn(current_batch_size, z_dim, 1, 1, device=device)\n",
        "            fake_imgs = G(z).cpu()\n",
        "            fake_imgs = (fake_imgs * 0.5) + 0.5\n",
        "            for j in range(fake_imgs.size(0)):\n",
        "                utils.save_image(fake_imgs[j], os.path.join(save_dir, f\"fake_{i+j}.png\"))\n",
        "    print(f\"Saved {num_images} generated images to {save_dir}\")\n",
        "\n",
        "save_generated_images(G, z_dim=z_dim, num_images=1000, batch_size=64, save_dir='generated_images_fid')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdTlAeUM8cWj"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def compute_fid(real_dir, fake_dir):\n",
        "    paths = [real_dir, fake_dir]\n",
        "    fid_value = fid_score.calculate_fid_given_paths(\n",
        "        paths,\n",
        "        batch_size=128,\n",
        "        device=device,\n",
        "        dims=2048\n",
        "    )\n",
        "    print(f\"FID: {fid_value}\")\n",
        "\n",
        "compute_fid(real_dir='real_images_subset', fake_dir='generated_images_fid')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BokCo87f8cWj"
      },
      "outputs": [],
      "source": [
        "def visualize_generated_images(G, z_dim=100, num_images=64):\n",
        "    G.eval()\n",
        "    with torch.no_grad():\n",
        "        sample_z = torch.randn(num_images, z_dim, 1, 1, device=device)\n",
        "        generated_imgs = G(sample_z).cpu()\n",
        "        generated_imgs = (generated_imgs * 0.5) + 0.5\n",
        "\n",
        "    plt.figure(figsize=(8,8))\n",
        "    grid = utils.make_grid(generated_imgs, nrow=8, normalize=False)\n",
        "    plt.imshow(grid.permute(1, 2, 0).numpy())\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "visualize_generated_images(G, z_dim=z_dim, num_images=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MHZkIIZ8cWj"
      },
      "outputs": [],
      "source": [
        "def compare_real_fake(real_dir='real_images_subset', fake_dir='generated_images_fid'):\n",
        "    real_imgs = sorted(glob.glob(os.path.join(real_dir, '*.png')))[:16]\n",
        "    fake_imgs = sorted(glob.glob(os.path.join(fake_dir, '*.png')))[:16]\n",
        "\n",
        "    plt.figure(figsize=(16, 8))\n",
        "    for i in range(16):\n",
        "        plt.subplot(4, 8, i + 1)\n",
        "        img = Image.open(real_imgs[i]).convert(\"RGB\")\n",
        "        img = np.array(img)\n",
        "        plt.imshow(img)\n",
        "        plt.axis(\"off\")\n",
        "        if i == 7:\n",
        "            plt.text(-10, 32, 'Real Images', fontsize=12, color='red')\n",
        "        plt.subplot(4, 8, i + 1 + 16)\n",
        "        img = Image.open(fake_imgs[i]).convert(\"RGB\")\n",
        "        img = np.array(img)\n",
        "        plt.imshow(img)\n",
        "        plt.axis(\"off\")\n",
        "        if i == 7:\n",
        "            plt.text(-10, 32, 'Generated Images', fontsize=12, color='blue')\n",
        "    plt.show()\n",
        "\n",
        "compare_real_fake(real_dir='real_images_subset', fake_dir='generated_images_fid')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUakNQL88cWj"
      },
      "source": [
        "### 5. Latent Space Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ut57CF9P8cWj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import utils\n",
        "\n",
        "def slerp(val, low, high):\n",
        "    omega = torch.acos(torch.clamp(torch.dot(low.flatten(), high.flatten()) /\n",
        "                                   (torch.norm(low) * torch.norm(high)), -1, 1))\n",
        "    so = torch.sin(omega)\n",
        "    if so == 0:\n",
        "        return (low + high) / 2\n",
        "    return (torch.sin((1.0 - val) * omega) / so) * low + (torch.sin(val * omega) / so) * high\n",
        "\n",
        "def interpolate_latent_space_slerp(G, z_dim=100, steps=8, device='cpu'):\n",
        "    G.eval()\n",
        "    with torch.no_grad():\n",
        "        z1 = torch.randn(1, z_dim, 1, 1, device=device)\n",
        "        z2 = torch.randn(1, z_dim, 1, 1, device=device)\n",
        "        alphas = np.linspace(0, 1, steps)\n",
        "        interpolated_imgs = []\n",
        "\n",
        "        for alpha in alphas:\n",
        "            z = slerp(alpha, z1, z2)\n",
        "            img = G(z).cpu()\n",
        "            interpolated_imgs.append(img[0])\n",
        "        interpolated_imgs = torch.stack(interpolated_imgs)\n",
        "\n",
        "        grid = utils.make_grid(interpolated_imgs, nrow=steps, normalize=True)\n",
        "\n",
        "        plt.figure(figsize=(steps * 2, 2))\n",
        "        plt.imshow(grid.permute(1, 2, 0).numpy())\n",
        "        plt.title(\"Latent Space Slerp Interpolation\")\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()\n",
        "\n",
        "interpolate_latent_space_slerp(G, z_dim=z_dim, steps=8, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsg6fBQO8cWj"
      },
      "source": [
        "\n",
        "이번 과제는 GAN을 이용해 CelebA 데이터셋을 학습 시키고, 이를 통해 가상의 얼굴 이미지를 생성하는것이 목적입니다.\n",
        "Inference 부분에서는 전체 과정에 대한 상세한 설명과 함께, 모델 아키텍처, 학습 과정, 평가 결과 등을 작성하였습니다.\n",
        "\n",
        "## 1. 모델 아키텍처\n",
        "\n",
        "### a. Generator\n",
        "Generator는 랜덤한 잠재 벡터\n",
        "𝑧\n",
        "z를 입력으로 받아 실제 데이터 분포와 유사한 이미지를 생성하는 역할을 수행합니다. 이 과정은 아래와 같은 단계로 이루어집니다:\n",
        "\n",
        "입력:\n",
        "\n",
        "𝑧\n",
        "z: 크기 100의 랜덤 잠재 벡터\n",
        "입력된\n",
        "𝑧\n",
        "z는 노이즈를 포함하며, 이를 기반으로 이미지를 생성합니다.\n",
        "ConvTranspose2d 레이어:\n",
        "\n",
        "역합성곱(Transpose Convolution): 잠재 벡터를 4x4x256의 텐서로 확장하고, 이를 반복적으로 업샘플링하여 최종적으로 64x64 크기의 이미지를 생성합니다.\n",
        "업샘플링 과정에서 채널 크기를 점차 줄이며 다음과 같은 해상도를 만듭니다:\n",
        "4\n",
        "×\n",
        "4\n",
        "4×4 →\n",
        "8\n",
        "×\n",
        "8\n",
        "8×8 →\n",
        "16\n",
        "×\n",
        "16\n",
        "16×16 →\n",
        "32\n",
        "×\n",
        "32\n",
        "32×32 →\n",
        "64\n",
        "×\n",
        "64\n",
        "64×64.\n",
        "BatchNorm2d:\n",
        "\n",
        "각 ConvTranspose2d 레이어 이후 배치 정규화를 적용하여 학습 안정성과 수렴 속도를 향상시킵니다.\n",
        "활성화 함수:\n",
        "\n",
        "중간 레이어에는 ReLU 활성화 함수가 사용되어 음수를 제거하고 비선형성을 도입합니다.\n",
        "최종 레이어에는 Tanh 활성화 함수가 사용되어 출력을 [-1, 1] 범위로 정규화합니다.\n",
        "출력:\n",
        "\n",
        "RGB 채널의 이미지를 생성하며 출력 크기는\n",
        "64\n",
        "×\n",
        "64\n",
        "×\n",
        "3\n",
        "64×64×3입니다.\n",
        "역전파 가능:\n",
        "\n",
        "ConvTranspose2d와 BatchNorm2d의 조합으로 Generator는 역전파가 효율적으로 이루어지도록 설계되어 있습니다.\n",
        "b. Discriminator\n",
        "Discriminator는 Generator가 생성한 가짜 이미지와 실제 이미지를 구분하는 역할을 합니다. 이미지의 진위를 판별하며, 아래와 같은 구조로 설계되었습니다:\n",
        "\n",
        "입력:\n",
        "\n",
        "𝑥\n",
        "x:\n",
        "64\n",
        "×\n",
        "64\n",
        "×\n",
        "3\n",
        "64×64×3 크기의 RGB 이미지\n",
        "실제 이미지와 가짜 이미지를 모두 입력받습니다.\n",
        "Conv2d 레이어:\n",
        "\n",
        "합성곱: 입력 이미지를 64x64에서 32x32, 16x16, 8x8, 4x4로 점진적으로 다운샘플링하며, 채널 수는 32 → 64 → 128 → 256으로 증가합니다.\n",
        "다운샘플링을 통해 고수준의 특징을 학습합니다.\n",
        "BatchNorm2d:\n",
        "\n",
        "각 Conv2d 레이어 이후 배치 정규화를 적용하여 학습 안정성과 성능을 향상시킵니다.\n",
        "활성화 함수:\n",
        "\n",
        "LeakyReLU 활성화 함수가 사용됩니다. 이 함수는 음수 기울기를 도입하여 0이 아닌 작은 값을 반환하며, 정보 손실을 줄이고 모델의 표현력을 향상시킵니다.\n",
        "최종 레이어:\n",
        "\n",
        "Conv2d로 4x4의 텐서를 1x1로 축소하며, Sigmoid 활성화 함수를 사용해 출력을 [0, 1] 범위로 정규화합니다.\n",
        "출력값은 이미지가 실제일 확률(0은 가짜, 1은 진짜)을 나타냅니다.\n",
        "출력:\n",
        "\n",
        "스칼라 값(이미지 진위 여부 확률)을 반환합니다.\n",
        "추가 설명\n",
        "Generator와 Discriminator의 관계: Generator는 Discriminator를 속이기 위해 더 실제와 유사한 이미지를 생성하려고 하며, Discriminator는 이 이미지를 구별하려고 학습합니다. 이 과정은 경쟁적(Adversarial)인 학습 과정으로, 두 모델이 서로 발전하면서 균형에 도달합니다.\n",
        "활성화 함수 선택:\n",
        "Generator의 최종 레이어에서 Tanh는 출력값을 [-1, 1]로 정규화하여 데이터 분포에 맞게 이미지를 생성하도록 돕습니다.\n",
        "Discriminator의 최종 레이어에서 Sigmoid는 확률로 변환해 진위 여부를 학습합니다.\n",
        "\n",
        "## 2. 학습 과정\n",
        "\n",
        "### a. 데이터 준비 및 전처리\n",
        "\n",
        "1. **데이터셋:** CelebA 데이터셋을 사용하여 얼굴 이미지 200,000장을 포함\n",
        "2. **전처리:**\n",
        "- 2.1. **크기 조정:** 모든 이미지를 64x64 픽셀로 리사이즈\n",
        "- 2.2. **텐서 변환:** 이미지를 텐서로 변환\n",
        "- 2.3. **정규화:** 픽셀 값을 [-1, 1] 범위로 정규화 (\\( \\text{Normalize}((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) \\))\n",
        "- 2.4. **DataLoader:** 배치 크기 64, 셔플링 적용하여 데이터 로드\n",
        "\n",
        "### b. 손실 함수 및 옵티마이저\n",
        "\n",
        "3. **손실 함수:** Binary Cross Entropy Loss (BCE Loss)\n",
        "- 3.1. **Discriminator:** 실제 이미지와 가짜 이미지를 분류\n",
        "- 3.2. **Generator:** Discriminator를 속이려는 목적으로 사용\n",
        "4. **옵티마이저:** Adam 옵티마이저 사용\n",
        "- 4.1. **학습률:** 0.0002\n",
        "- 4.2. **베타 값:** (0.5, 0.999)\n",
        "\n",
        "### c. 레이블 스무딩\n",
        "\n",
        "- 5.1. **진짜 라벨:** 1 대신 0.9로 설정하여 Discriminator의 자신이 진짜 사람이라는 것을 인식하는것을 방지하기 위해 사용\n",
        "- 5.2. **가짜 라벨:** 0으로 설정\n",
        "\n",
        "### d. 학습 단계\n",
        "\n",
        "- **에포크 수:** 50\n",
        "- **훈련 단계:**\n",
        "  1. **Discriminator 학습:**\n",
        "     - 실제 이미지와 가짜 이미지를 통해 Discriminator의 손실 계산\n",
        "     - 합산된 손실을 역전파하여 Discriminator의 가중치 업데이트\n",
        "  2. **Generator 학습:**\n",
        "     - 가짜 이미지를 통해 Discriminator의 출력을 받아 Generator의 손실 계산 (진짜로 속이려는 목표)\n",
        "     - 손실을 역전파하여 Generator의 가중치 업데이트\n",
        "\n",
        "### e. 모델 저장\n",
        "\n",
        "- **모델 저장:** 모든 에포크가 완료된 후, Generator와 Discriminator의 가중치를 저장하여 추후 평가 및 재사용이 가능하도록 함\n",
        "\n",
        "## 3. 아쉬운 점과 향후 개선 방향\n",
        "\n",
        "1. **FID 점수**: 현재 FID 점수는 아쉬운 수준입니다. 이는 생성된 이미지의 품질과 다양성이 실제 이미지와 상당한 차이가 있음을 의미하고 있습니다. 더 많은 에포크로 학습하고 모델 구조를 개선하면 더 좋은 결과를 얻을 수 있을 것 같습니다.\n",
        "\n",
        "2. **학습 시간**: 컴퓨팅 자원의 한계로 충분한 에포크 수로 학습하지 못한 것이 아쉽습니다. 현재 FID 점수를 보면 모델이 충분히 수렴하지 못했다는 것을 알 수 있습니다. GPU 환경에서 더 긴 시간 학습하면 더 나은 결과를 얻을 수 있을 것 같다고 생각됩니다.\n",
        "\n",
        "3. **이미지 품질**: 생성된 이미지들이 아직 많이 흐릿하고 세부 디테일이 부족합니다. StyleGAN과 같은 최신 아키텍처를 적용하면 더 좋은 품질의 이미지를 생성할 수 있을 것 같다고 생각됩니다."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}